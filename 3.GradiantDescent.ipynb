{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent **optmisation technique**, uses **cost and iteration** to learn the patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> this image is of gradient descent **CONVEX function** \n",
    "Data and Model decides what image(CONVEX function) we will get, it's not upto us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](Images/gradientdescentcurve.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost is **Error**\n",
    "\n",
    "W is Epochs or **Iteration**\n",
    "\n",
    "Learning steps is **Size of Itereations/Epochs**\n",
    "\n",
    "**0.001 to 0.5** is a good range for ***learning rate***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we get closer to the point of minimum error the distance covered by the epochs get smaller and smaller to minimise error between two epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets understand from example of linear regression\n",
    "\n",
    "We randomly select values of m and c in mx+x and then we will find the errors and then change the values of m and c which give least error.\n",
    "\n",
    "predicted - actual = error is underestood as cost, higher the error higher the cost.\n",
    "and gradient descent tries to reduce cost by iteration and changing values \n",
    "basically works as binary search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How will Gradient Descent know how far ro which direction it is from the point of least error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets assume we are using MSE to calculate error \n",
    "\n",
    "the effecting values in case of **MSE** are **m** and **c** in **mx+c** formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt](Images/errorinmse.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^yi = y=mx+c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate partial derivatives:\n",
    "- With respect to M\n",
    "\n",
    "    ![alt](Images/derivedm.jfif)\n",
    "- With respect to C\n",
    "\n",
    "    ![alt](Images/derivedc.jfif)\n",
    "- new values of m and c\n",
    "\n",
    "    ![alt](Images/newmandc.jfif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ways to judge errors(Cost functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE Mean Square Error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "MAE Mean Absolute Error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "SSE Sum Of Squared Error   ssr Sum Of Squared Residual\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
